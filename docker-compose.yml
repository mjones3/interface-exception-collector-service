version: '3.8'

services:
  # PostgreSQL Database for Interface Exception Collector
  postgres:
    image: postgres:15-alpine
    container_name: exception-collector-db
    environment:
      POSTGRES_DB: exception_collector_db
      POSTGRES_USER: exception_user
      POSTGRES_PASSWORD: exception_pass
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --lc-collate=C --lc-ctype=C"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U exception_user -d exception_collector_db"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - exception-collector-network

  # PostgreSQL Database for Partner Order Service
  postgres-partner:
    image: postgres:15-alpine
    container_name: partner-order-db
    environment:
      POSTGRES_DB: partner_order_db
      POSTGRES_USER: partner_order_user
      POSTGRES_PASSWORD: partner_order_pass
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --lc-collate=C --lc-ctype=C"
    ports:
      - "5433:5432"
    volumes:
      - postgres_partner_data:/var/lib/postgresql/data
      - ./scripts/init-partner-db.sql:/docker-entrypoint-initdb.d/init-partner-db.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U partner_order_user -d partner_order_db"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - exception-collector-network

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: exception-collector-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    networks:
      - exception-collector-network

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: exception-collector-kafka
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    ports:
      - "29092:29092"
      - "9092:9092"
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    restart: unless-stopped
    networks:
      - exception-collector-network

  # Kafka UI for development
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: exception-collector-kafka-ui
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      DYNAMIC_CONFIG_ENABLED: 'true'
    ports:
      - "8081:8080"
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - exception-collector-network

  # Kafka Topic Initialization
  kafka-init:
    image: confluentinc/cp-kafka:7.4.0
    container_name: exception-collector-kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    command: |
      bash -c "
        echo 'Creating Kafka topics...'
        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic OrderRejected
        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic ValidationError
        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic ExceptionCaptured
        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic ExceptionResolved
        kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic CriticalExceptionAlert
        echo 'Kafka topics created successfully!'
      "
    networks:
      - exception-collector-network

  # Interface Exception Collector Service
  exception-collector:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    container_name: exception-collector-service
    environment:
      # Spring profiles
      SPRING_PROFILES_ACTIVE: docker
      
      # Database configuration
      SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/exception_collector_db
      SPRING_DATASOURCE_USERNAME: exception_user
      SPRING_DATASOURCE_PASSWORD: exception_pass
      
      # Kafka configuration
      SPRING_KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      SPRING_KAFKA_CONSUMER_GROUP_ID: interface-exception-collector
      
      # Redis configuration
      SPRING_REDIS_HOST: redis
      SPRING_REDIS_PORT: 6379
      
      # JVM configuration
      JAVA_OPTS: >-
        -XX:+UseContainerSupport
        -XX:MaxRAMPercentage=75.0
        -XX:+UseG1GC
        -XX:+UseStringDeduplication
        -Djava.security.egd=file:/dev/./urandom
        -Dspring.profiles.active=docker
        
      # Application configuration
      SERVER_PORT: 8080
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,metrics,prometheus
      MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS: always
      
      # Logging configuration
      LOGGING_LEVEL_COM_ARCONE_BIOPRO: DEBUG
      LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_KAFKA: INFO
      LOGGING_PATTERN_CONSOLE: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{correlationId:-}] %logger{36} - %msg%n"
    ports:
      - "8080:8080"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - exception-collector-network
    volumes:
      - app_logs:/app/logs

  # Partner Order Service
  partner-order-service:
    build:
      context: ./partner-order-service
      dockerfile: Dockerfile
      target: runtime
    container_name: partner-order-service
    environment:
      # Spring profiles
      SPRING_PROFILES_ACTIVE: docker
      
      # Database configuration
      DB_HOST: postgres-partner
      DB_PORT: 5432
      DB_NAME: partner_order_db
      DB_USERNAME: partner_order_user
      DB_PASSWORD: partner_order_pass
      
      # Kafka configuration
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      
      # JVM configuration
      JAVA_OPTS: >-
        -XX:+UseContainerSupport
        -XX:MaxRAMPercentage=75.0
        -XX:+UseG1GC
        -XX:+UseStringDeduplication
        -Djava.security.egd=file:/dev/./urandom
        -Dspring.profiles.active=docker
        
      # Application configuration
      SERVER_PORT: 8090
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,metrics,prometheus
      MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS: always
      
      # Logging configuration
      LOG_LEVEL_APP: DEBUG
      LOG_LEVEL_KAFKA: INFO
      LOGGING_PATTERN_CONSOLE: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{correlationId:-}] %logger{36} - %msg%n"
    ports:
      - "8090:8090"
    depends_on:
      postgres-partner:
        condition: service_healthy
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - exception-collector-network
    volumes:
      - partner_app_logs:/app/logs

  # Prometheus for metrics collection (optional for development)
  prometheus:
    image: prom/prometheus:latest
    container_name: exception-collector-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    depends_on:
      - exception-collector
    restart: unless-stopped
    networks:
      - exception-collector-network
    profiles:
      - monitoring

  # Grafana for metrics visualization (optional for development)
  grafana:
    image: grafana/grafana:latest
    container_name: exception-collector-grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - exception-collector-network
    profiles:
      - monitoring

volumes:
  postgres_data:
    driver: local
  postgres_partner_data:
    driver: local
  redis_data:
    driver: local
  kafka_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  app_logs:
    driver: local
  partner_app_logs:
    driver: local

networks:
  exception-collector-network:
    driver: bridge
    name: exception-collector-network